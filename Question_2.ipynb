{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - Pyspark\n",
    "\n",
    "### Spark Environment Check\n",
    "Run below code to check if your intial environment configuration using `bash set_local_env.bash` worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/11/04 08:42:55 WARN Utils: Your hostname, D275063 resolves to a loopback address: 127.0.1.1; using 172.18.132.34 instead (on interface eth0)\n",
      "23/11/04 08:42:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/04 08:42:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14151744\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "x, y, z = 1, 2, 3\n",
    "\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_val for var_name, var_val in callers_local_vars if var_val is var]\n",
    "\n",
    "print(retrieve_name(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1932/524182153.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     \u001b[0mcallers_local_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvar_val\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallers_local_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvar_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_col_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_original_time_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1932/524182153.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_original_time_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     _col_time_ingested = (\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_ingested'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimestampType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time_ingested'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         )\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mColumn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_invoke_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/functions.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0mInvokes\u001b[0m \u001b[0mJVM\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColumn\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mjf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_jvm_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F, types as T, Window\n",
    "import inspect\n",
    "POC_INTERVAL_LENGTH = 3\n",
    "\n",
    "def get_original_time_columns():\n",
    "    col_list = []\n",
    "    _col_time_ingested = (\n",
    "        F.col('time_ingested')\n",
    "        .cast(T.TimestampType())\n",
    "        .alias('time_ingested')\n",
    "        )\n",
    "    window = Window.partitionBy(_col_time_ingested)\n",
    "    _col_end_time_pst = (\n",
    "        F.col('datetime')\n",
    "        .cast('timestamp')\n",
    "        .alias('end_datetime_pst')\n",
    "    )\n",
    "    _col_min_time_ingested = (\n",
    "        F.least(\n",
    "            _col_time_ingested,\n",
    "            F.min(_col_end_time_pst).over(window)\n",
    "        )\n",
    "    )\n",
    "    _col_start_time = (\n",
    "        _col_end_time_pst -\n",
    "        F.expr(\n",
    "            'INTERVAL ' + str(POC_INTERVAL_LENGTH) + ' HOURS'\n",
    "        ).alias('datetime_pst')\n",
    "    )\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_val for var_name, var_val in callers_local_vars if var_val.startswith(\"_col_\")]\n",
    "\n",
    "col_list = get_original_time_columns()\n",
    "for col in col_list:\n",
    "    print(col)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scenario\n",
    "A 70 years old and very oddly placed [silo](https://en.wikipedia.org/wiki/Silo) in a refinery circuit is void of digital weight / volume gauge sensors and needs an operator to climb up and [measure the silo content manually](https://github.com/adilBaigSCE/SCE-Innovation-Lab/blob/main/assets/data/silo_actuals.csv) (both for weight and volume). \n",
    "\n",
    "![refinary_silo.png](assets/images/refinary_silo.png)\n",
    "\n",
    "Due to limited resources and ever increasing higher priority concerns in the plant, this manual measurement takes place randomly as and how it permits to refinery operation. There is a [historical average from past data](assets/data/historical_averages.csv) of how much product is available in the silo any given day of the week. Also the Refinary operation's Inventory Reconcilation team's \"Weekly Team Huddle\" meeting is every thursday, that report \"weekly inventory\" on a custom week scale _(last thursday to this wednesday)_\n",
    "Using these two data points, and using pyspark, print a monthly report that reads exactly like the [output_reference.csv](https://github.com/adilBaigSCE/SCE-Innovation-Lab/blob/main/assets/data/output_reference.csv):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Data below\n",
    "\n",
    "##### Actual Silo Readings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>silo_wt_in_tons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/5/2023</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/6/2023</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/10/2023</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/14/2023</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6/21/2023</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6/24/2023</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6/25/2023</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6/30/2023</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  silo_wt_in_tons\n",
       "0   6/1/2023               70\n",
       "1   6/5/2023               53\n",
       "2   6/6/2023               62\n",
       "3  6/10/2023               43\n",
       "4  6/14/2023               66\n",
       "5  6/21/2023               78\n",
       "6  6/24/2023               41\n",
       "7  6/25/2023               35\n",
       "8  6/30/2023               83"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"assets/data/silo_actuals.csv\", sep=\",\", keep_default_na=False)\n",
    "df.head(31)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Tons from historical daily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>average_tons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monday</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Friday</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         day  average_tons\n",
       "0     Monday            50\n",
       "1    Tuesday            65\n",
       "2  Wednesday            70\n",
       "3   Thursday            80\n",
       "4     Friday            65\n",
       "5   Saturday            45\n",
       "6     Sunday            40"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"assets/data/historical_averages.csv\", sep=\",\", keep_default_na=False)\n",
    "df.head(31)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a pyspark code, to generate output report table __for the entire month of June, 2023__ that has following columns _with these coding restrictions_:\n",
    "* __silo_wt_in_tons__ : \n",
    "    * use actual measured tons where available\n",
    "    * fall back to historical average on dates its not available\n",
    "    * <em>(Optional) SPARK CHALLENGE: DO NOT use sequential for loop to back fill dates between <strong>6/1/2023</strong> through <strong>6/30/2023</strong></em>\n",
    "* __weekly_total_tons__ :\n",
    "    * use a custom week starting from thursday ending in wednesday next week\n",
    "    * report total sum of tons against last day of the week i.e. wednesday\n",
    "    * weekly total can cross monthly boundaries, i.e. span across two months.\n",
    "    * <em>(Optional) SPARK CHALLENGE: DO NOT use seperate dataset to calculate groupBy sum fo tons for the week and then join that in</em>\n",
    "* __mtd_running_total_tons__ :\n",
    "    * A running total of tons each day in a monthly window\n",
    "* __monthly_grand_total__ :\n",
    "    * the total tons incoming per month.\n",
    "* <strong>Following coding restrictions below are GOOD TO HAVE and will showcase your advanced spark skills, but dont let that stop you in finding solution even if you cannot!</strong>\n",
    "* <em>CODING RESTRICTIONS</em>\n",
    "    * As these are small dataset, do not worry about skewing or partitioning best practice for now\n",
    "    * A production ready code with good documentation and comments would score exrta points\n",
    "    * Write clean [pythonic code](https://docs.python-guide.org/writing/style/) as much as possible\n",
    "    * OPTIONAL: Fully endorse [functional programming](https://realpython.com/python-functional-programming/) for seperating of concern if you can\n",
    "    * AVOID USING UDF FUNCTION FOR ANY CACULATIONS\n",
    "    * Use the new spark 3.4.* [pyspark.sql.DataFrame.transform](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html#pyspark.sql.DataFrame.transform) to write cleaner code\n",
    "\n",
    "\n",
    "The output should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>silo_wt_in_tons</th>\n",
       "      <th>weekly_total_tons</th>\n",
       "      <th>mtd_running_total_tons</th>\n",
       "      <th>monthly_grand_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>70</td>\n",
       "      <td></td>\n",
       "      <td>70</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6/2/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>135</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6/3/2023</td>\n",
       "      <td>45</td>\n",
       "      <td></td>\n",
       "      <td>180</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6/4/2023</td>\n",
       "      <td>40</td>\n",
       "      <td></td>\n",
       "      <td>220</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6/5/2023</td>\n",
       "      <td>53</td>\n",
       "      <td></td>\n",
       "      <td>273</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6/6/2023</td>\n",
       "      <td>62</td>\n",
       "      <td></td>\n",
       "      <td>335</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6/7/2023</td>\n",
       "      <td>70</td>\n",
       "      <td>405</td>\n",
       "      <td>405</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6/8/2023</td>\n",
       "      <td>80</td>\n",
       "      <td></td>\n",
       "      <td>485</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6/9/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>550</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6/10/2023</td>\n",
       "      <td>43</td>\n",
       "      <td></td>\n",
       "      <td>593</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6/11/2023</td>\n",
       "      <td>40</td>\n",
       "      <td></td>\n",
       "      <td>633</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td>683</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6/13/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>748</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6/14/2023</td>\n",
       "      <td>66</td>\n",
       "      <td>409</td>\n",
       "      <td>814</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6/15/2023</td>\n",
       "      <td>80</td>\n",
       "      <td></td>\n",
       "      <td>894</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6/16/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>959</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6/17/2023</td>\n",
       "      <td>45</td>\n",
       "      <td></td>\n",
       "      <td>1004</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6/18/2023</td>\n",
       "      <td>40</td>\n",
       "      <td></td>\n",
       "      <td>1044</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6/19/2023</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td>1094</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6/20/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>1159</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6/21/2023</td>\n",
       "      <td>78</td>\n",
       "      <td>423</td>\n",
       "      <td>1237</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6/22/2023</td>\n",
       "      <td>80</td>\n",
       "      <td></td>\n",
       "      <td>1317</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6/23/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>1382</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6/24/2023</td>\n",
       "      <td>41</td>\n",
       "      <td></td>\n",
       "      <td>1423</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6/25/2023</td>\n",
       "      <td>35</td>\n",
       "      <td></td>\n",
       "      <td>1458</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6/26/2023</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td>1508</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6/27/2023</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>1573</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6/28/2023</td>\n",
       "      <td>70</td>\n",
       "      <td>406</td>\n",
       "      <td>1643</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6/29/2023</td>\n",
       "      <td>80</td>\n",
       "      <td></td>\n",
       "      <td>1723</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6/30/2023</td>\n",
       "      <td>83</td>\n",
       "      <td></td>\n",
       "      <td>1806</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  silo_wt_in_tons weekly_total_tons  mtd_running_total_tons  \\\n",
       "0    6/1/2023               70                                        70   \n",
       "1    6/2/2023               65                                       135   \n",
       "2    6/3/2023               45                                       180   \n",
       "3    6/4/2023               40                                       220   \n",
       "4    6/5/2023               53                                       273   \n",
       "5    6/6/2023               62                                       335   \n",
       "6    6/7/2023               70               405                     405   \n",
       "7    6/8/2023               80                                       485   \n",
       "8    6/9/2023               65                                       550   \n",
       "9   6/10/2023               43                                       593   \n",
       "10  6/11/2023               40                                       633   \n",
       "11  6/12/2023               50                                       683   \n",
       "12  6/13/2023               65                                       748   \n",
       "13  6/14/2023               66               409                     814   \n",
       "14  6/15/2023               80                                       894   \n",
       "15  6/16/2023               65                                       959   \n",
       "16  6/17/2023               45                                      1004   \n",
       "17  6/18/2023               40                                      1044   \n",
       "18  6/19/2023               50                                      1094   \n",
       "19  6/20/2023               65                                      1159   \n",
       "20  6/21/2023               78               423                    1237   \n",
       "21  6/22/2023               80                                      1317   \n",
       "22  6/23/2023               65                                      1382   \n",
       "23  6/24/2023               41                                      1423   \n",
       "24  6/25/2023               35                                      1458   \n",
       "25  6/26/2023               50                                      1508   \n",
       "26  6/27/2023               65                                      1573   \n",
       "27  6/28/2023               70               406                    1643   \n",
       "28  6/29/2023               80                                      1723   \n",
       "29  6/30/2023               83                                      1806   \n",
       "\n",
       "   monthly_grand_total  \n",
       "0                       \n",
       "1                       \n",
       "2                       \n",
       "3                       \n",
       "4                       \n",
       "5                       \n",
       "6                       \n",
       "7                       \n",
       "8                       \n",
       "9                       \n",
       "10                      \n",
       "11                      \n",
       "12                      \n",
       "13                      \n",
       "14                      \n",
       "15                      \n",
       "16                      \n",
       "17                      \n",
       "18                      \n",
       "19                      \n",
       "20                      \n",
       "21                      \n",
       "22                      \n",
       "23                      \n",
       "24                      \n",
       "25                      \n",
       "26                      \n",
       "27                      \n",
       "28                      \n",
       "29                      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"assets/data/output_reference.csv\", sep=\",\", keep_default_na=False)\n",
    "# shows top 10 rows\n",
    "df.head(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
